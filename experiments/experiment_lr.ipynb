{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T14:27:35.444217900Z",
     "start_time": "2023-10-29T14:27:35.426214600Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import platform\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T14:36:54.887456500Z",
     "start_time": "2023-10-29T14:36:54.879459400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "from calendar import EPOCH\n",
    "\n",
    "\n",
    "ROOT_DIR = os.path.join(os.getcwd(), \"..\")\n",
    "USE_FULLDATA = False\n",
    "RESTRECTED_FEATURES = False\n",
    "RANDOM_SEED = 2023\n",
    "N_SPLITS= 4\n",
    "ACTIVATION = 'relu'\n",
    "ENCODER_SIZES = [10, 5]\n",
    "Model_type = 'LogisticRegression'\n",
    "TIME_STAMP = datetime.datetime.now().__str__()\n",
    "\n",
    "# AutoEncoder\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# 使用する機械学習モデル\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "params = {\n",
    "    'penalty': 'l2',\n",
    "    'solver': 'lbfgs',\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'max_iter': 200\n",
    "}\n",
    "Model = LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T14:27:35.490721500Z",
     "start_time": "2023-10-29T14:27:35.476725400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT DIRECTORY:  /Users/rsato/ml/experiments/..\n",
      "USE:  10% data\n",
      "ran:  2023-11-06 15:53:36.250860\n",
      "python:      3.9.6\n",
      "sklearn:     1.3.1\n",
      "tensorflow:  2.14.0\n",
      "keras:       2.14.0\n",
      "numpy:       1.26.1\n",
      "pandas:      2.1.1\n"
     ]
    }
   ],
   "source": [
    "print(\"ROOT DIRECTORY: \", ROOT_DIR)\n",
    "print(\"USE: \", \"Full data\" if USE_FULLDATA else \"10% data\")\n",
    "print(\"ran: \", datetime.datetime.now())\n",
    "print(f\"python:      {platform.python_version()}\")\n",
    "print(f\"sklearn:     {sklearn.__version__}\")\n",
    "print(f\"tensorflow:  {tf.__version__}\")\n",
    "print(f\"keras:       {keras.__version__}\")\n",
    "print(f\"numpy:       {np.__version__}\")\n",
    "print(f\"pandas:      {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T14:27:37.372207Z",
     "start_time": "2023-10-29T14:27:35.494721800Z"
    }
   },
   "outputs": [],
   "source": [
    "# KDD'99 ラベルデータの読み込み\n",
    "with open(ROOT_DIR + \"/datasets/kddcup.names\", \"r\") as f:\n",
    "        # 一行目は不要なので無視\n",
    "    _ = f.readline()\n",
    "    # `:`より手前がラベルなので，その部分を抽出してリストに追加\n",
    "    names = [line.split(':')[0] for line in f]\n",
    "# 　正解ラベルを追加\n",
    "names.append(\"true_label\")\n",
    "\n",
    "# KDD'99 クラスラベルデータの読み込み\n",
    "with open(ROOT_DIR + \"/datasets/training_attack_types\", \"r\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "    classes = {'normal': 'normal'}\n",
    "    for line in lines:\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        k, v = tuple(line.split(\" \"))\n",
    "        classes[k] = v\n",
    "\n",
    "# 除外する特徴量のリスト\n",
    "ignore_names = [\n",
    "    \"hot\", \"num_compromised\", \"num_file_creations\",      \n",
    "    \"num_outbound_cmds\", \"is_host_login\", \"srv_count\",\n",
    "    \"srv_serror_rate\", \"srv_rerror_rate\", \"same_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\",\n",
    "    \"dst_host_diff_srv_rate\"\n",
    "    ]\n",
    "\n",
    "# KDD'99 データの読み込み\n",
    "if USE_FULLDATA:\n",
    "    df = pd.read_csv(ROOT_DIR + \"/datasets/kddcup.data\", names=names, index_col=False)\n",
    "else:\n",
    "    df = pd.read_csv(ROOT_DIR + \"/datasets/kddcup.data_10_percent\", names=names, index_col=False)\n",
    "\n",
    "# カテゴリー特徴量を削除\n",
    "data_x: pd.DataFrame = df.copy().drop(columns=['protocol_type', 'service', 'flag'], axis=1)\n",
    "\n",
    "# 除外する特徴量を削除\n",
    "if RESTRECTED_FEATURES:\n",
    "    data_x = data_x.drop(columns=ignore_names, axis=1)\n",
    "\n",
    "\n",
    "# ラベルデータを切り分ける\n",
    "data_y = data_x.pop(\"true_label\").map(lambda x: x.replace('.', ''))\n",
    " \n",
    "# namesを更新\n",
    "names = data_x.columns\n",
    "\n",
    " # 正規化\n",
    "data_x = pd.DataFrame(StandardScaler().fit_transform(data_x), columns=names)\n",
    "\n",
    "# ラベルを変換\n",
    "data_y = data_y.map(lambda x: classes[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T14:27:37.388205900Z",
     "start_time": "2023-10-29T14:27:37.372207Z"
    }
   },
   "outputs": [],
   "source": [
    "# k分割\n",
    "k_fold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T14:28:49.954446400Z",
     "start_time": "2023-10-29T14:27:37.391204600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder0 (Dense)            (None, 10)                390       \n",
      "                                                                 \n",
      " encoder1 (Dense)            (None, 5)                 55        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                60        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 38)                418       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 923 (3.61 KB)\n",
      "Trainable params: 923 (3.61 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "15439/15439 [==============================] - 8s 511us/step - loss: 0.5884\n",
      "Epoch 2/5\n",
      "15439/15439 [==============================] - 8s 505us/step - loss: 0.5361\n",
      "Epoch 3/5\n",
      "15439/15439 [==============================] - 8s 514us/step - loss: 0.5266\n",
      "Epoch 4/5\n",
      "15439/15439 [==============================] - 8s 507us/step - loss: 0.5192\n",
      "Epoch 5/5\n",
      "15439/15439 [==============================] - 8s 506us/step - loss: 0.5040\n",
      "15439/15439 [==============================] - 5s 302us/step\n"
     ]
    }
   ],
   "source": [
    "def generate_encoder(x: pd.DataFrame):\n",
    "    _model = keras.Sequential( \n",
    "        [\n",
    "            Dense(ENCODER_SIZES[0], activation=ACTIVATION, input_shape=(x.shape[1],), name=\"encoder0\"),\n",
    "            *[\n",
    "                Dense(hidden_layer_size, activation=ACTIVATION, name=f\"encoder{idx + 1}\")\n",
    "                for idx, hidden_layer_size in enumerate(ENCODER_SIZES[1:])\n",
    "            ],\n",
    "            *[\n",
    "                Dense(hidden_layer_size, activation=ACTIVATION)\n",
    "                for hidden_layer_size in ENCODER_SIZES[-2::-1]\n",
    "            ],\n",
    "            Dense(x.shape[1], activation=ACTIVATION),\n",
    "        ]\n",
    "    )\n",
    "    _model.summary()\n",
    "    _model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    _model.fit(x, x, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "    return keras.Sequential(_model.layers[: len(ENCODER_SIZES)])\n",
    "\n",
    "encoder = generate_encoder(data_x)\n",
    "new_features = pd.DataFrame(encoder.predict(data_x),\n",
    "columns=[f\"ae_{idx}\" for idx in range(ENCODER_SIZES[-1])])\n",
    "# データを結合\n",
    "data_x_ae = pd.concat([data_x, new_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T14:28:49.969444600Z",
     "start_time": "2023-10-29T14:28:49.954446400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((494021, 38), (494021, 43))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x.shape, data_x_ae.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T14:28:50.014965400Z",
     "start_time": "2023-10-29T14:28:49.972445200Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(_x, _y):\n",
    "    _generator = k_fold.split(_x, _y)\n",
    "    accuracies = []\n",
    "    for fold, (train_idx, test_idx) in enumerate(_generator):\n",
    "        print(f\"fold: {fold}\")\n",
    "        # データを分割\n",
    "        x_train = _x.iloc[train_idx]\n",
    "        y_train = _y.iloc[train_idx]\n",
    "        x_test = _x.iloc[test_idx]\n",
    "        y_test = _y.iloc[test_idx]\n",
    "\n",
    "        # モデルを学習\n",
    "        model = Model(**params)\n",
    "        model.fit(x_train, y_train)\n",
    "        # テストデータで評価\n",
    "        accuracy = classification_report(y_test, model.predict(x_test), output_dict=True)\n",
    "        accuracies.append(accuracy)\n",
    "        print(f\"f1-score: {accuracy['macro avg']['f1-score']}\") # type: ignore\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T14:29:20.206902600Z",
     "start_time": "2023-10-29T14:28:49.992450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 0\n",
      "f1-score: 0.8236682584036096\n",
      "fold: 1\n",
      "f1-score: 0.9012716436456458\n",
      "fold: 2\n",
      "f1-score: 0.8603551030159677\n",
      "fold: 3\n",
      "f1-score: 0.8904327944441512\n",
      "fold: 0\n",
      "f1-score: 0.8248071500194417\n",
      "fold: 1\n",
      "f1-score: 0.8867096362916012\n",
      "fold: 2\n",
      "f1-score: 0.8351745882409741\n",
      "fold: 3\n",
      "f1-score: 0.880757815285049\n"
     ]
    }
   ],
   "source": [
    "# 元の特徴量のみで学習\n",
    "results_default = predict(data_x, data_y)\n",
    "\n",
    "# AE特徴量を追加して学習\n",
    "results_ae = predict(data_x_ae, data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T16:02:20.176802700Z",
     "start_time": "2023-10-29T16:02:20.162804200Z"
    }
   },
   "outputs": [],
   "source": [
    "results = dict()\n",
    "results['config'] = {\n",
    "    'USE_FULLDATA': USE_FULLDATA,\n",
    "    'RESTRECTED_FEATURES': RESTRECTED_FEATURES,\n",
    "    'RANDOM_SEED': RANDOM_SEED,\n",
    "    'N_SPLITS': N_SPLITS,\n",
    "    'Date': TIME_STAMP,\n",
    "    'ACTIVATION': ACTIVATION,\n",
    "    'ENCODER_SIZES': ENCODER_SIZES,\n",
    "    'EPOCHS': EPOCHS,\n",
    "    'BATCH_SIZE': BATCH_SIZE,\n",
    "    'Model': Model_type,\n",
    "    'params': params\n",
    "}\n",
    "results['default'] = dict()\n",
    "for k0 in ['default', 'ae']:\n",
    "    results[k0] = dict()\n",
    "    if k0 == 'default':\n",
    "        result = results_default\n",
    "    else:\n",
    "        result = results_ae\n",
    "    for k1 in results_default[0].keys():\n",
    "        results[k0][k1] = dict()\n",
    "        if not hasattr(results_default[0][k1], 'keys'):\n",
    "            continue\n",
    "        for k2 in results_default[0][k1].keys():\n",
    "            if k2 == 'support':\n",
    "                results[k0][k1][k2] = np.sum([result[i][k1][k2] for i in range(4)])\n",
    "            else:\n",
    "                results[k0][k1][k2] = np.mean([result[i][k1][k2] for i in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T16:02:20.743395800Z",
     "start_time": "2023-10-29T16:02:20.728395800Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(f'{ROOT_DIR}/results/{Model_type}{ENCODER_SIZES}-{TIME_STAMP}.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-29T14:29:20.318425500Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-29T14:29:20.319426600Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
