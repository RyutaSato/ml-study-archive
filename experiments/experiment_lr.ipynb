{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T14:27:35.444217900Z",
     "start_time": "2023-10-29T14:27:35.426214600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rsato/ml/.venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import platform\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T14:36:54.887456500Z",
     "start_time": "2023-10-29T14:36:54.879459400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "DEBUG = False\n",
    "\n",
    "from calendar import EPOCH\n",
    "\n",
    "DATASET_NAME = 'kdd99'\n",
    "AUTOENCODER_USED_DATA = 'all' # all, normal, anomaly, dos, probe, u2r, r2l\n",
    "USE_FULLDATA = False\n",
    "DESCRIPTION = 'None'\n",
    "\n",
    "ROOT_DIR = os.path.join(os.getcwd(), \"..\")\n",
    "RESTRECTED_FEATURES = False\n",
    "RANDOM_SEED = 2023\n",
    "N_SPLITS= 4\n",
    "ACTIVATION = 'relu'\n",
    "ENCODER_SIZES = [10, 5]\n",
    "Model_type = 'LogisticRegression'\n",
    "TIME_STAMP = datetime.datetime.now(tz=datetime.timezone(datetime.timedelta(hours=9)))\n",
    "\n",
    "# AutoEncoder\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# 使用する機械学習モデル\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "Model = LogisticRegression\n",
    "params = {\n",
    "    'penalty': 'l2',\n",
    "    'solver': 'lbfgs',\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'max_iter': 200\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T14:27:35.490721500Z",
     "start_time": "2023-10-29T14:27:35.476725400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT DIRECTORY:  /Users/rsato/ml/experiments/..\n",
      "USE:  10% data\n",
      "ran:  2023-11-08 16:43:20.428923+09:00\n",
      "python:      3.9.6\n",
      "sklearn:     1.3.1\n",
      "tensorflow:  2.14.0\n",
      "keras:       2.14.0\n",
      "numpy:       1.26.1\n",
      "pandas:      2.1.1\n"
     ]
    }
   ],
   "source": [
    "print(\"ROOT DIRECTORY: \", ROOT_DIR)\n",
    "print(\"USE: \", \"Full data\" if USE_FULLDATA else \"10% data\")\n",
    "print(\"ran: \", TIME_STAMP)\n",
    "print(f\"python:      {platform.python_version()}\")\n",
    "print(f\"sklearn:     {sklearn.__version__}\")\n",
    "print(f\"tensorflow:  {tf.__version__}\")\n",
    "print(f\"keras:       {keras.__version__}\")\n",
    "print(f\"numpy:       {np.__version__}\")\n",
    "print(f\"pandas:      {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T14:27:37.372207Z",
     "start_time": "2023-10-29T14:27:35.494721800Z"
    }
   },
   "outputs": [],
   "source": [
    "# KDD'99 ラベルデータの読み込み\n",
    "with open(ROOT_DIR + \"/datasets/kddcup.names\", \"r\") as f:\n",
    "        # 一行目は不要なので無視\n",
    "    _ = f.readline()\n",
    "    # `:`より手前がラベルなので，その部分を抽出してリストに追加\n",
    "    names = [line.split(':')[0] for line in f]\n",
    "# 　正解ラベルを追加\n",
    "names.append(\"true_label\")\n",
    "\n",
    "# KDD'99 クラスラベルデータの読み込み\n",
    "with open(ROOT_DIR + \"/datasets/training_attack_types\", \"r\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "    classes = {'normal': 'normal'}\n",
    "    for line in lines:\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        k, v = tuple(line.split(\" \"))\n",
    "        classes[k] = v\n",
    "\n",
    "# 除外する特徴量のリスト\n",
    "ignore_names = [\n",
    "    \"hot\", \"num_compromised\", \"num_file_creations\",      \n",
    "    \"num_outbound_cmds\", \"is_host_login\", \"srv_count\",\n",
    "    \"srv_serror_rate\", \"srv_rerror_rate\", \"same_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\",\n",
    "    \"dst_host_diff_srv_rate\"\n",
    "    ]\n",
    "category_names = [\"protocol_type\", \"service\", \"flag\"]\n",
    "\n",
    "# KDD'99 データの読み込み\n",
    "if USE_FULLDATA:\n",
    "    df = pd.read_csv(ROOT_DIR + \"/datasets/kddcup.data\", names=names, index_col=False)\n",
    "else:\n",
    "    df = pd.read_csv(ROOT_DIR + \"/datasets/kddcup.data_10_percent\", names=names, index_col=False)\n",
    "\n",
    "# カテゴリー特徴量を削除\n",
    "data_x: pd.DataFrame = df.copy().drop(columns=category_names, axis=1)\n",
    "\n",
    "# 除外する特徴量を削除\n",
    "if RESTRECTED_FEATURES:\n",
    "    data_x = data_x.drop(columns=ignore_names, axis=1)\n",
    "\n",
    "\n",
    "# ラベルデータを切り分ける\n",
    "data_y = data_x.pop(\"true_label\").map(lambda x: x.replace('.', ''))\n",
    " \n",
    "# namesを更新\n",
    "names = data_x.columns\n",
    "\n",
    " # 正規化\n",
    "data_x = pd.DataFrame(StandardScaler().fit_transform(data_x), columns=names)\n",
    "\n",
    "# ラベルを変換\n",
    "data_y = data_y.map(lambda x: classes[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T14:27:37.388205900Z",
     "start_time": "2023-10-29T14:27:37.372207Z"
    }
   },
   "outputs": [],
   "source": [
    "# k分割\n",
    "k_fold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T14:28:49.954446400Z",
     "start_time": "2023-10-29T14:27:37.391204600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder0 (Dense)            (None, 10)                390       \n",
      "                                                                 \n",
      " encoder1 (Dense)            (None, 5)                 55        \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                60        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 38)                418       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 923 (3.61 KB)\n",
      "Trainable params: 923 (3.61 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "15439/15439 [==============================] - 8s 499us/step - loss: 0.5973\n",
      "Epoch 2/5\n",
      "15439/15439 [==============================] - 8s 495us/step - loss: 0.5342\n",
      "Epoch 3/5\n",
      "15439/15439 [==============================] - 8s 493us/step - loss: 0.5057\n",
      "Epoch 4/5\n",
      "15439/15439 [==============================] - 8s 496us/step - loss: 0.4909\n",
      "Epoch 5/5\n",
      "15439/15439 [==============================] - 8s 493us/step - loss: 0.4930\n",
      "15439/15439 [==============================] - 5s 295us/step\n"
     ]
    }
   ],
   "source": [
    "def generate_encoder(x: pd.DataFrame):\n",
    "    if ENCODER_SIZES is None:\n",
    "        return None\n",
    "    _model = keras.Sequential( \n",
    "        [\n",
    "            Dense(ENCODER_SIZES[0], activation=ACTIVATION, input_shape=(x.shape[1],), name=\"encoder0\"),\n",
    "            *[\n",
    "                Dense(hidden_layer_size, activation=ACTIVATION, name=f\"encoder{idx + 1}\")\n",
    "                for idx, hidden_layer_size in enumerate(ENCODER_SIZES[1:])\n",
    "            ],\n",
    "            *[\n",
    "                Dense(hidden_layer_size, activation=ACTIVATION)\n",
    "                for hidden_layer_size in ENCODER_SIZES[-2::-1]\n",
    "            ],\n",
    "            Dense(x.shape[1], activation=ACTIVATION),\n",
    "        ]\n",
    "    )\n",
    "    _model.summary()\n",
    "    _model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    _model.fit(x, x, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "    return keras.Sequential(_model.layers[: len(ENCODER_SIZES)])\n",
    "\n",
    "if ENCODER_SIZES is not None:\n",
    "    encoder = generate_encoder(data_x)\n",
    "    new_features = pd.DataFrame(encoder.predict(data_x),\n",
    "    columns=[f\"ae_{idx}\" for idx in range(ENCODER_SIZES[-1])])\n",
    "    # データを結合\n",
    "    data_x = pd.concat([data_x, new_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T14:28:49.969444600Z",
     "start_time": "2023-10-29T14:28:49.954446400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(494021, 43)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T14:28:50.014965400Z",
     "start_time": "2023-10-29T14:28:49.972445200Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(_x, _y):\n",
    "    _generator = k_fold.split(_x, _y)\n",
    "    accuracies = []\n",
    "    for fold, (train_idx, test_idx) in enumerate(_generator):\n",
    "        print(f\"fold: {fold}\")\n",
    "        # データを分割\n",
    "        x_train = _x.iloc[train_idx]\n",
    "        y_train = _y.iloc[train_idx]\n",
    "        x_test = _x.iloc[test_idx]\n",
    "        y_test = _y.iloc[test_idx]\n",
    "\n",
    "        # モデルを学習\n",
    "        model = Model(**params)\n",
    "        model.fit(x_train, y_train)\n",
    "        # テストデータで評価\n",
    "        accuracy = classification_report(y_test, model.predict(x_test), output_dict=True)\n",
    "        accuracies.append(accuracy)\n",
    "        print(f\"f1-score: {accuracy['macro avg']['f1-score']}\") # type: ignore\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T14:29:20.206902600Z",
     "start_time": "2023-10-29T14:28:49.992450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 0\n",
      "f1-score: 0.8262941811949478\n",
      "fold: 1\n",
      "f1-score: 0.9245261298720449\n",
      "fold: 2\n",
      "f1-score: 0.8743471668141941\n",
      "fold: 3\n",
      "f1-score: 0.9164669584476229\n"
     ]
    }
   ],
   "source": [
    "# 学習\n",
    "results = predict(data_x, data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T16:02:20.176802700Z",
     "start_time": "2023-10-29T16:02:20.162804200Z"
    }
   },
   "outputs": [],
   "source": [
    "outputs = {\n",
    "    'feature_size': data_x.shape[1],\n",
    "    'dropped': RESTRECTED_FEATURES,\n",
    "    'seed': RANDOM_SEED,\n",
    "    'splits': N_SPLITS,\n",
    "    'datetime': TIME_STAMP,\n",
    "    'dataset': {\n",
    "        'name': DATASET_NAME,\n",
    "        'description': DESCRIPTION,\n",
    "        'ratio': 1 if USE_FULLDATA else 0.1,\n",
    "        'size': data_x.shape[1] - ENCODER_SIZES[-1] if ENCODER_SIZES is not None else data_x.shape[1],\n",
    "    },\n",
    "    'autoencoder': {\n",
    "        'used_data': AUTOENCODER_USED_DATA,\n",
    "        'layers': ENCODER_SIZES,\n",
    "        'epochs': EPOCHS,\n",
    "        'activation': ACTIVATION,\n",
    "        'batch_size': BATCH_SIZE\n",
    "    },\n",
    "    'model': {\n",
    "        'type': Model_type,\n",
    "        **params\n",
    "    }\n",
    "}\n",
    "outputs['result'] = dict()\n",
    "for k1 in results[0].keys():\n",
    "    outputs['result'][k1] = dict()\n",
    "    if not hasattr(results[0][k1], 'keys'):\n",
    "        continue\n",
    "    for k2 in results[0][k1].keys():\n",
    "        if k2 == 'support':\n",
    "            outputs['result'][k1][k2] = np.sum([results[i][k1][k2] for i in range(4)])\n",
    "        else:\n",
    "            outputs['result'][k1][k2] = np.mean([results[i][k1][k2] for i in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "if not DEBUG:\n",
    "    from pymongo.mongo_client import MongoClient\n",
    "    from pymongo.server_api import ServerApi\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    uri = os.getenv('MongoDBURI')\n",
    "\n",
    "    # Create a new client and connect to the server\n",
    "    client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "\n",
    "    # Send a ping to confirm a successful connection\n",
    "    client.admin.command('ping')\n",
    "    db = client.get_database('ml')\n",
    "    assert db is not None, \"db is None\"\n",
    "    collection = db.get_collection('results')\n",
    "    assert collection is not None, \"collection is None\"\n",
    "    _result = collection.insert_one(outputs)\n",
    "    if _result is None:\n",
    "        print(\"successfully reflected results to DB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DEBUG:\n",
    "    assert Model_type in locals().keys(), f\"Model_type: {Model_type} does not match\"\n",
    "    dropped = \"-d\" if RESTRECTED_FEATURES else \"\"\n",
    "    fname = f'{ROOT_DIR}/results/{DATASET_NAME}{dropped}.{Model_type}.{ENCODER_SIZES}.json'\n",
    "    outputs['datetime'] = str(outputs['datetime'])\n",
    "    del outputs['_id']\n",
    "    with open(fname, 'w') as f:\n",
    "        json.dump(outputs, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
