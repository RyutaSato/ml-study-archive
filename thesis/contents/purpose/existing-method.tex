\section{既存手法のアプローチ}

不均衡データにおいて，少数派クラスの分類精度を改善する一般的な手法として，以下の3つが挙げられる．

\begin{enumerate}
    \item アンダーサンプリング \\
    多数派クラスのサンプルを削減することで，データセットの不均衡を解消する手法である．
    この手法の利点として，データセットのサイズが小さくなるため，学習における計算時間が短縮されることが挙げられる．一方で，データの削減により，有用な情報が失われる可能性がある．
    \item オーバーサンプリング\\
    少数派クラスのサンプルを増加させることで，データセットの不均衡を解消する手法である．
    有名な手法として，SMOTE(Synthetic Minority Oversampling Technique)\cite{smote}が挙げられる．
    SMOTEは，少数派クラスのサンプルを増加させる際に，少数派クラスのサンプルの近傍に新たなサンプルを生成する．この手法の問題点は，近似のサンプルを繰り返し学習させることによる過学習の可能性があること，そして学習時間が増加することである．
    \item 少数派クラスの損失関数のコスト（重み）をより大きくする手法\\
    少数派クラスの誤分類に対するコストを大きくすることで，少数派クラスの分類精度を改善する手法である．クラスのサンプル数の逆数を重みとして使用されることが多い．
\end{enumerate}

これらの既存手法は，サンプル数やコストを変えるという多数派クラスと少数派クラスの学習における重要度を等しくするという点で共通している．